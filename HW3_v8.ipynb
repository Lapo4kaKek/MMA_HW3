{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3v5HIUdDvY5"
   },
   "source": [
    "# HSE 2023: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 3\n",
    "\n",
    "**Warning 1**: some problems require (especially the lemmatization part) significant amount of time, so **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F7t9dYtdDvZC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install end\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "print(\"install end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIHtwV6vDvZD"
   },
   "source": [
    "## PART 1: Logit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7XKEWcVDvZD"
   },
   "source": [
    "We consider a binary classification problem. For prediction, we would like to use a logistic regression model. For regularization we add a combination of the $l_2$ and $l_1$ penalties (Elastic Net).\n",
    "\n",
    "Each object in the training dataset is indexed with $i$ and described by pair: features $x_i\\in\\mathbb{R}^{K}$ and binary labels $y_i$. The model parametrized with bias $w_0\\in\\mathbb{R}$ and weights $w\\in\\mathbb{R}^K$. Note: Bias is included in $w$ vector\n",
    "\n",
    "The optimization problem with respect to the $w_0, w$ is the following (Logistic loss with Elastic Net regularizers):\n",
    "\n",
    "$$L(w, w_0) = \\sum_{i=1}^{N} -y_i \\log{\\sigma{(w^\\top x_i)}} - (1 - y_i) \\log{(1 - \\sigma{(w^\\top x_i)})} + \\gamma \\|w\\|_1 + \\beta \\|w\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1eSuDKXFVZu"
   },
   "source": [
    "#### 1. [0.5 points]  Find the gradient of the Elastic Net loss and write its formulas (better in latex format). Remember what derivative sigmoid has (gradient in fact is a lot simpler than you may get using automatic tools like sympy, matlab or whatever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zjH-YnPDvZD"
   },
   "source": [
    "##### найдем производную сигмоидной функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сразу реализуем сигмоидную функцию активации \n",
    "$$ \\frac{1}{e^{-value} + 1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value):\n",
    "    return 1 / (np.exp(-value) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тогда градиент логистической части функции потерь по весу w_j для j-го признака будет:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\sum_{i=1}^{N} (y_i - \\sigma(w^T x_i))x_{ij}$$\n",
    "#### Градиент регуляризационной части L1(Lasso) по весу w_j\n",
    "$$\\frac{\\partial}{\\partial w_j} \\gamma \\|w\\|_1 = \\gamma \\cdot \\text{sign}(w_j)\n",
    "$$\n",
    "#### Градиент регуляризационной части L2(Ridge) по весу w_j\n",
    "$$\\frac{\\partial}{\\partial w_j} \\beta \\|w\\|^2_2 = 2 \\cdot \\beta \\cdot w_j\n",
    "$$\n",
    "#### Объединяем:\n",
    "$$\\nabla_w L(w) = \\sum_{i=1}^{N} (y_i - \\sigma(w^T x_i))x_{i} + \\gamma \\cdot \\text{sign}(w) + 2 \\cdot \\beta \\cdot w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_lIccN_DvZE"
   },
   "source": [
    "#### 2. [0.25 points] Implement the Elastic Net loss (as a function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9QNfCtV5DvZE"
   },
   "outputs": [],
   "source": [
    "#def loss(X, y, w: List[float], gamma=1., beta=1.) -> float:\n",
    "#    sigmoid()\n",
    "def loss(X, y, w:List[float], gamma=1.0, beta=1.0):\n",
    "    w = np.array(w)\n",
    "    \n",
    "    predictions = sigmoid(np.dot(X, w))\n",
    "    \n",
    "    log_loss = -np.mean(y * np.log(predictions + 1e-15) + (1 - y) * np.log(1 - predictions + 1e-15))\n",
    "    \n",
    "    l1_penalty = gamma * np.sum(np.abs(w))\n",
    "    \n",
    "    l2_penalty = beta * np.sum(w ** 2)\n",
    "    \n",
    "    total_loss = log_loss + l1_penalty + l2_penalty\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIVoC6UmDvZE"
   },
   "source": [
    "#### 3. [0.25 points] Implement the gradient (as a function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Возвращаемся к пункту 1:\n",
    "$$\\nabla_w L(w) = \\sum_{i=1}^{N} (y_i - \\sigma(w^T x_i))x_{i} + \\gamma \\cdot \\text{sign}(w) + 2 \\cdot \\beta \\cdot w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HWqBLGRADvZE"
   },
   "outputs": [],
   "source": [
    "#def get_grad(X, y, w: List[float], gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
    "#    grad_w = #...\n",
    "\n",
    "#    return grad_w\n",
    "\n",
    "def get_grad(X, y, w: List[float], gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
    "    sigma = sigmoid(np.dot(X, w))\n",
    "    \n",
    "    grad_w = np.dot(X.T, (sigma - y)) + gamma * np.sign(w) + 2 * beta * w\n",
    "    \n",
    "    return grad_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhOb8HrtDvZF"
   },
   "source": [
    "#### Check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3FxXTocHDvZF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi PAul!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.multivariate_normal(np.arange(5), np.eye(5), size=10)\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "y = np.random.binomial(1, 0.42, size=10)\n",
    "w = np.random.normal(size=5 + 1)\n",
    "\n",
    "grad_w = get_grad(X, y, w)\n",
    "assert(np.allclose(grad_w,\n",
    "                   [-3.99447493, -1.84786723,  0.64520104,  1.67059973, -5.03858487, -5.21496336],\n",
    "                   rtol=1e-2)\n",
    ")\n",
    "print(\"Hi PAul!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbqLfcrRDvZF"
   },
   "source": [
    "####  4. [1 point]  Implement gradient descent which works for both tol level and max_iter stop criteria and plot the decision boundary of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIgiwQkjDvZF"
   },
   "source": [
    "The template provides basic sklearn API class. You are free to modify it in any convenient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Thyeux0KDvZG"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I-mPUA6YaEH"
   },
   "outputs": [],
   "source": [
    "class Logit(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, beta=1.0, gamma=1.0, lr=1e-3, tolerance=0.01, max_iter=1000, random_state=42):\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tolerance= tolerance\n",
    "        self.max_iter= max_iter\n",
    "        self.learning_rate = lr\n",
    "        self.random_state = random_state\n",
    "        self.w = None\n",
    "        # you may additional properties if you wish\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # add weights and bias and optimize Elastic Net loss over (X,y) dataset\n",
    "        # save history of optimization steps\n",
    "\n",
    "        # your code here\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # return vector of predicted labels (0 or 1) for each object from X\n",
    "        # your code here\n",
    "\n",
    "        return predict\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "      # return vector of probabilities for each object from X\n",
    "        return np.array([1 / (1 + np.exp(np.dot(X, self.w))),\\\n",
    "                         1 / (1 + np.exp(-np.dot(X, self.w)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SJX8Y6EDvZG"
   },
   "outputs": [],
   "source": [
    "# sample data to test your model\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=180, n_features=2, n_redundant=0, n_informative=2,\n",
    "                               random_state=42, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u41kzwGTDvZH"
   },
   "outputs": [],
   "source": [
    "# a function to plot the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    fig = plt.figure()\n",
    "    X1min, X2min = X.min(axis=0)\n",
    "    X1max, X2max = X.max(axis=0)\n",
    "    x1, x2 = np.meshgrid(np.linspace(X1min, X1max, 200),\n",
    "                         np.linspace(X2min, X2max, 200))\n",
    "    ypred = model.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "    ypred = ypred.reshape(x1.shape)\n",
    "\n",
    "    plt.contourf(x1, x2, ypred, alpha=.4)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNuYbsAoDvZI"
   },
   "outputs": [],
   "source": [
    "model = Logit(0, 0)\n",
    "model.fit(X, y)\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi4WRhcADvZI"
   },
   "source": [
    "#### 5. [0.25 points] Plot loss diagram for the model, i.e. show the dependence of the loss function from the gradient descent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyjMDAKuDvZI"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FhSCAv_DvZJ"
   },
   "source": [
    "## PART 2: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYyGsSxEDvZJ"
   },
   "source": [
    "#### 6. [2 point] Using the same dataset, train SVM Classifier from Sklearn.\n",
    "Investigate how different parameters influence the quality of the solution:\n",
    "+ Try several kernels: Linear, Polynomial, RBF (and others if you wish). Some Kernels have hypermeters: don't forget to try different.\n",
    "+ Regularization coefficient\n",
    "\n",
    "Show how these parameters affect accuracy, roc_auc and f1 score.\n",
    "Make plots for the dependencies between metrics and parameters.\n",
    "Try to formulate conclusions from the observations. How sensitive are kernels to hyperparameters? How sensitive is a solution to the regularization? Which kernel is prone to overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nicu_O3IDvZK"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY8q6JdCDvZK"
   },
   "source": [
    "## PART 3: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD4xKhYfDvZK"
   },
   "source": [
    "#### 7. [1.75 point] Form the dataset\n",
    "\n",
    "We are going to form a dataset that we will use in the following tasks for binary and multiclass classification\n",
    "\n",
    "0. Choose **six** authors that you like (specify who you've chosen) and download the <a href=\"https://www.kaggle.com/d0rj3228/russian-literature?select=prose\">relevant data</a> from **prose** section\n",
    "1. Build your own dataset for these authors:\n",
    "    * divide each text into sentences such that we will have two columns: *sentence* and *target author*, each row will contain one sentence and one target\n",
    "    * drop sentences where N symbols in a sentence < 15\n",
    "    * fix random state and randomly choose sentences in the folowing proportion \"5k : 15k : 8k : 11k : 20k : 3k\" for the authors respectively\n",
    "    \n",
    "    sample data may look like:\n",
    "    \n",
    "    <center>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th> sentence </th>\n",
    "            <th> author </th>\n",
    "        </tr>\n",
    "        <tr><td> Несколько лет тому назад в одном из своих поместий жил старинный русской барин, Кирила Петрович Троекуров. </td><td> Пушкин </td><td>\n",
    "        <tr><td> Уже более недели приезжий господин жил в городе, разъезжая по вечеринкам и обедам и таким образом проводя, как говорится, очень приятно время. </td><td> Гоголь </td><td>\n",
    "        <tr><td> ... </td><td> ... </td><td>\n",
    "        <tr><td> Я жил недорослем, гоняя голубей и играя в чехарду с дворовыми мальчишками. </td><td> Пушкин </td><td>         \n",
    "    </table>\n",
    "</center>\n",
    "     \n",
    "2. Preprocess (tokenize and clean) the dataset\n",
    "    * tokenize, remove all stop words (nltk.corpus.stopwords), punctuation (string.punctuation) and numbers\n",
    "    * convert to lower case and apply either stemming or lemmatization of the words (on your choice)\n",
    "    * vectorize words using both **bag of words** and **tf-idf** (use sklearn)\n",
    "    * observe and describe the difference between vectorized output (what do numbers look like after transformations and what do they represent?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVStbeQ8DvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuTi3rvnDvZL"
   },
   "source": [
    "###  Binary classification\n",
    "\n",
    "#### 8. [2 point] Train model using Logistic Regression (your own) and SVC (SVM can be taken from sklearn)\n",
    "\n",
    "* choose *two* authors from the dataset that you have formed in the previous task\n",
    "* check the balance of the classes\n",
    "* divide the data into train and test samples with 0.7 split rate (don't forget to fix the random state)\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score) and use it in the next tasks\n",
    "* make several plots to address the dependence between F1 score and parameters\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute some relevant metrics for test sample (useful to check the seminars 5 and 6, use sklearn)\n",
    "* make conclusions about the performance of your models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZUP1HqFDvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G1kt0qbDvZL"
   },
   "source": [
    "#### 9. [1 point] Analysing ROC AUC\n",
    "\n",
    "It is possible to control the proportion of statistical errors of different types using different thresholds for choosing a class. Plot ROC curves for Logistic Regression and SVC, show the threshold on ROC curve plots. Choose such a threshold that your models have no more than 30% of false positive errors rate. Pay attention to `thresholds` parameter in sklearn roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ2GZ8-uDvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-qubaK4DvZM"
   },
   "source": [
    "### Multiclass logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJQone-LDvZM"
   },
   "source": [
    "#### 10. [1 point] Take the One-VS-One classifier (use sklearn) and apply to Logit model (one you've made in the 4th task) in order to get multiclass linear classifier\n",
    "\n",
    "*It is possible to use sklearn model instead of your own one but with a penalty of 0.5*\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html\">OneVsOneClassifier</a>\n",
    "\n",
    "* use the data you got at the previous step for 6 authors\n",
    "* divide the data into train and test samples with 0.7 split rate\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score)\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute relevant metrics for test sample (use sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4lR8qJ7DvZM"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW3_v7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
